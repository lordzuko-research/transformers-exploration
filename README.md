# transformers-exploration
Code for exploration of fundamentals and capabilities of various transformer based models

## Transformer Variants

### Vanilla Transformers

![Vanilla Transformers](https://github.com/lordzuko-research/transformers-exploration/blob/main/imgs/survey_transfomers_1.jpg?raw=true "Source: A Survey of Transformers")

### X-formers

The vanilla transformer was a simple encoder-decoder architecture, further research into this have identified that the stack of encoder which was used there is is a powerful model, capable of learning various semantic, syntactic aspects of language similar to the earlier work in ImageNet in computer vision. This led to wide adaptation of transformers especially in the rapidly growing space of transfer learning in deeplearning based NLP models.

Here is a quick overview of these modifications which has been done with the architectures in various research. It is evident that the architecture has various parts which can be *added-removed-modified* to create a new **X-former**.

![X-formers](https://github.com/lordzuko-research/transformers-exploration/blob/main/imgs/survey_transfomers_2.jpg?raw=true "Source: A Survey of Transformers")


## References
* [A Survey of Transformers](https://arxiv.org/pdf/2106.04554.pdf)
