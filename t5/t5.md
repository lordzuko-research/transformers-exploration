[T5 - Text-to-Text Transfer Transformer](https://arxiv.org/pdf/1910.10683.pdf)

## How is t5's language modeling different from X-formers

*  BERT doesn't do text generation, we can do that with T5
* Unlike X-formers/BERT which gets trained on masked language modeling
  or next sentence prediction, T5 was trained on multiple tasks where 
  tasks were differentiated by input prefix.
* To finetune a model, we need create dataset with added task prefix to the input